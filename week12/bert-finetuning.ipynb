{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "bert-finetuning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/it3103/blob/main/week12/bert-finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIaeZuPBfwya"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti107/blob/master/session-8/bert-finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>\n",
        "\n",
        "<br/>\n",
        "\n",
        "# Fine-tuning BERT for Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrE6epjAfwyd"
      },
      "source": [
        "One of the approaches where we can use BERT for downstream task such as text classification is to do fine-tuning of the pretrained model. \n",
        "\n",
        "In this lab, we will see how we can use a pretrained DistilBert Model and fine-tune it with custom training data for text classification task. \n",
        "\n",
        "At the end of this session, you will be able to:\n",
        "- prepare data and use model-specific Tokenizer to format data suitable for use by the model\n",
        "- configure the transformer model for fine-tuning \n",
        "- train the model for binary and multi-class text classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqmw-ZHoBw1I"
      },
      "source": [
        "### Install Hugging Face Transformers library\n",
        "\n",
        "If you are running this notebook in Google Colab, you will need to install the Hugging Face transformers library as it is not part of the standard environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRVZlas4f4NK"
      },
      "source": [
        "!pip install transformers==4.6.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVC0VH5Rfwyd"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    TFAutoModelForSequenceClassification,\n",
        "    TFTrainer,\n",
        "    TFTrainingArguments,\n",
        "    TFDistilBertForSequenceClassification\n",
        ")\n",
        "from transformers.utils import logging as hf_logging\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We enable logging level to info and use default log handler and log formatting\n",
        "hf_logging.set_verbosity_info()\n",
        "hf_logging.enable_default_handler()\n",
        "hf_logging.enable_explicit_format()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbfUyhxsfwye"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di5Bf6u2fwye"
      },
      "source": [
        "# Uncomment the following if you have not downloaded the datasets.\n",
        "\n",
        "# !wget https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_test.csv\n",
        "# !wget https://nyp-aicourse.s3-ap-southeast-1.amazonaws.com/datasets/imdb_train.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNjl2iy8fwyf"
      },
      "source": [
        "train_df = pd.read_csv('imdb_train.csv')\n",
        "test_df = pd.read_csv('imdb_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhcDPN0wCsWi"
      },
      "source": [
        "The train set has 40000 samples. We will a small subset (e.g. 2000) samples for finetuning our pretrained model. Similarly we will use a smaller test set for evaluating our model.  We use dataframe's `sample()` to randomly select a subset of samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeFtqmAEfwyf"
      },
      "source": [
        "TRAIN_SIZE = 2000\n",
        "TEST_SIZE = 200 \n",
        "\n",
        "train_df = train_df.sample(n=TRAIN_SIZE)\n",
        "test_df = test_df.sample(n=TEST_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODwiiebLfwyf"
      },
      "source": [
        "train_df['sentiment'] =  train_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)\n",
        "test_df['sentiment'] =  test_df['sentiment'].apply(lambda x: 0 if x == 'negative' else 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d1tQUKNfwyg"
      },
      "source": [
        "train_df.sentiment.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzBCEG1efwyg"
      },
      "source": [
        "train_texts = train_df['review']\n",
        "train_labels = train_df['sentiment']\n",
        "test_texts = test_df['review']\n",
        "test_labels = test_df['sentiment']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsQnWEA2fwyh"
      },
      "source": [
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM49b__4fwyh"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "We will now load the DistilBert tokenizer for the pretrained model \"distillbert-base-cased\".  The tokenizer helps to produce the input tokens that are suitable to be used by the model, e.g. it automatically append the \\[CLS\\] token in the front of the sentence and the \\[SEP\\] token at the end of the token, and also the attention mask for those padded positions in the input sequence of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hd492GPqfwyh"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME-wLa1yfwyh"
      },
      "source": [
        "The DistilBERT tokenizer (identical to Bert tokenizer) use WordPiece vocabulary. It has close to 30000 words and it maps pretrained embeddings for each. Each word has its own ids, we would need to map the tokens to those ids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFAN7yoZfwyi"
      },
      "source": [
        "print(f\"Tokenizer vocab size = {tokenizer.vocab_size}\")\n",
        "print(list(tokenizer.vocab.keys())[6000:6020])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOs5f0Idfwyi"
      },
      "source": [
        "Let us take a closer look at the output of the tokenization process. \n",
        "\n",
        "We notice that the tokenizer will return a dictionary of two items 'input_ids' and 'attention_mask'. The input_ids contains the IDs of the tokens. While the 'attention_mask' contains the masking pattern for those padding. If you are using BERT tokenizer, there will be additional item called 'token_type_ids'.\n",
        "\n",
        "We also notice that for the example sentence, the word 'Transformer' is being broken up into two tokens 'Trans' and '##former'. Similarly, 'Processing' is tokenized as 'Process' and '##ing'.  The '##' means that the rest of the token should be attached to the previous one.\n",
        "\n",
        "We also see that the tokenizer appended \\[CLS\\] to the beginning of the token sequence, and \\[SEP\\] at the end. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS2hQw8ifwyi"
      },
      "source": [
        "test_sentence = \"Transformer is really good for Natural Language Processing.\"\n",
        "\n",
        "encoding = tokenizer(test_sentence, padding=True, truncation=True)\n",
        "print(f\"Encoding keys:  {encoding.keys()}\\n\")\n",
        "\n",
        "print(f\"token ids: {encoding['input_ids']}\\n\")\n",
        "\n",
        "print(f\"tokens: {tokenizer.convert_ids_to_tokens(encoding['input_ids'])}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_npR61rfwyj"
      },
      "source": [
        "Now let's go ahead and tokenize our texts. But before we do so, we need to convert the pandas series to list first as the tokenizer cannot work with pandas series or dataframe directly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFDiYuDTfwyj"
      },
      "source": [
        "train_texts = train_texts.to_list()\n",
        "train_labels = train_labels.to_list()\n",
        "val_texts = val_texts.to_list()\n",
        "val_labels = val_labels.to_list()\n",
        "test_texts = test_texts.to_list()\n",
        "test_labels = test_labels.to_list()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23BuebaNfwyj"
      },
      "source": [
        "train_encodings = tokenizer(train_texts, padding=True, truncation=True)\n",
        "val_encodings = tokenizer(val_texts, padding=True, truncation=True)\n",
        "test_encodings = tokenizer(test_texts, padding=True, truncation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygaYFdjfwyk"
      },
      "source": [
        "We then create a tf dataset using the encodings and the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrJBnhuXfwyk"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZWzPjzQfwyk"
      },
      "source": [
        "## Fine-tuning the model\n",
        "\n",
        "Now let us fine-tune the pre-trained model by training it with our custom dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi4-umnlfwyl"
      },
      "source": [
        "We will now instantiate a pretrained model 'distilbert-base-uncased', using `TFAutoModelForSequenceClassification`, and passing `num_labels=2` to indicate we want a binary classification. \n",
        "\n",
        "The model is a `tf.keras.Model` subclass. So you can train the model using Keras API such as `fit()`, or use Tensorflow custom training loops if you want to have more control over the training. The transformer library however, provides a Trainer class which abstract away the complex training loop, and supports distributed training on multi-GPU system. We will use this to train our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwCWbjeqfwyl"
      },
      "source": [
        "To use the Trainer class, we need to setup the training arguments such as number of epochs, batch sizes, warming up steps (commonly used in training Transformer model), weight decay (used to by Adam Optimizer for regularization purpose), learning rate, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mg8QONG7fwyl"
      },
      "source": [
        "training_args = TFTrainingArguments(\"./my_train\", \n",
        "                                    evaluation_strategy='steps',\n",
        "                                    eval_steps=50,\n",
        "                                    num_train_epochs=1,\n",
        "                                    logging_steps=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlHuehIKfwym"
      },
      "source": [
        "with training_args.strategy.scope():\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-uncased\",num_labels=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_V76-cZvfwym"
      },
      "source": [
        "We then define a function `compute_metrics()`  that will be used to compute metrics at evaluation. it takes in a EvalPrediction and return a dictionary string to metric values. In our case we just return the accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz01grVWfwym"
      },
      "source": [
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return {\"acc\": (preds == p.label_ids).mean()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfOSV4zBfwym"
      },
      "source": [
        "# We define a tensorboard writer \n",
        "writer = tf.summary.create_file_writer(\"tblogs\")\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model, \n",
        "    args=training_args, \n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_dataset, \n",
        "    eval_dataset=val_dataset,\n",
        "    tb_writer=writer\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci79f1pMfwym"
      },
      "source": [
        "We start the training, and do the evaluation. On a single-GPU system, the training will around 6-7 minutes to complete. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7zJMRydfwyn"
      },
      "source": [
        "trainer.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbzrVh0VGEgG"
      },
      "source": [
        "%reload_ext tensorboard \n",
        "%tensorboard --logdir tblogs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_L3bBZlfwyn"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RpchDiofwyn"
      },
      "source": [
        "Let's see how it performs on our test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuR5s85bfwyn"
      },
      "source": [
        "preds = trainer.predict(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZEYjvu9fwyo"
      },
      "source": [
        "The output from predict is logits, so we need to use a softmax to turn the values to probabilities and then use np.argmax to select the label with largest probalities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBR1ShX1fwyp"
      },
      "source": [
        "tf_predictions = tf.nn.softmax(preds.predictions, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u1owmeNfwyp"
      },
      "source": [
        "y_preds = np.argmax(tf_predictions, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkc2jJDnfwyp"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(preds.label_ids, y_preds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8obURu4fwyp"
      },
      "source": [
        "#model.save_pretrained('./save_model/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVMSvuVBfwyp"
      },
      "source": [
        "## Try out the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZXWuxUufwyp"
      },
      "source": [
        "Now let's try out our model with our own sentence. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeBzO0YMfwyq"
      },
      "source": [
        "test_sentence = \"I don't see how people can sit through this hour-long movie!\"\n",
        "#test_sentence = \"The movie, though flawed, is still interesting enough.\"\n",
        "test_sentence = 'The movie kept we on my toe all the time.'\n",
        "inputs = tokenizer(test_sentence, return_tensors=\"tf\")\n",
        "out = model(inputs)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBwdTBxxmgPK"
      },
      "source": [
        "print(np.argmax(tf.nn.softmax(out.logits, axis=-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh0X1-Dtfwyq"
      },
      "source": [
        "**Exercise:**\n",
        "\n",
        "- Try to use BERT base-cased pretrained model and see if you get better or worse performance.\n",
        "- Try to use BERT base-uncased pretrained model and see if you get better or worse performance.\n",
        "- Try using a larger number of training samples. \n",
        "- Try multi-class classification using the this [dataset](https://sdaai-bucket.s3-ap-southeast-1.amazonaws.com/datasets/news.csv) that groups news title into 4 categories: e (entertainment), b (business), t (tech), m (medical/health). Original dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdcjl7t2fwyq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}